<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VLA Model-Expert Collaboration for Bi-directional Manipulation Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VLA Model-Expert Collaboration for Bi-directional Manipulation
              Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Tian-Yu Xiang</a><sup>†</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/AoqunJin" target="_blank">Ao-Qun Jin</a><sup>†</sup>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Xiao-Hu Zhou</a><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Mei-Jiang Gui</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Xiao-Liang Xie</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Shi-Qi Liu</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Shuang-Yi Wang</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Sheng-Bin Duang</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Si-Cheng Wang</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Zheng Lei</a>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Zeng-Guang Hou</a><sup>*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Institute of Automation, Chinese Academy of Sciences</span>
              <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.04163" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.04163" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/Intro.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered"></h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The emergence of vision-language-action (VLA) models has given rise to foundation models for robot
              manipulation. Although these models have achieved significant improvements, their generalization in
              multi-task manipulation remains limited. This study proposes a VLA model-expert collaboration framework
              that leverages a limited number of expert actions to enhance VLA model performance. This approach reduces
              expert workload relative to manual operation while simultaneously improving the reliability and
              generalization of VLA models. Furthermore, manipulation data collected during collaboration can further
              refine the VLA model, while human participants concurrently enhance their skills. This bi-directional
              learning loop boosts the overall performance of the collaboration system. Experimental results across
              various VLA models demonstrate the effectiveness of the proposed system in collaborative manipulation and
              learning, as evidenced by improved success rates across tasks. Additionally, validation using a
              brain-computer interface (BCI) indicates that the collaboration system enhances the efficiency of
              low-speed action systems by involving VLA model during manipulation. These promising results pave the way
              for advancing human-robot interaction in the era of foundation models for robotics.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/image1.png" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          <div class="content has-text-justified">Figure 1. The proposed VLA model-expert collaboration system
            integrates a VLA model and expert interactions to enhance manipulation. The VLA model generates actions by
            processing task instructions as text tokens and environmental inputs as vision tokens. Meanwhile, the expert
            makes decisions at a lower frequency, assisting the VLA model. Expert-executed actions are collected to
            fine-tune the VLA model, improving system performance.</div>
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/image2.png" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          <div class="content has-text-justified">Figure 2. Collaboration pipeline between VLA model and expert for
            manipulation and learning.</div>
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/image5.png" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          <div class="content has-text-justified">Success rate of collaboration between VLA models and rule-based expert
            policy under different ratios (VLA/expert) in MT10 and MT50 benchmarks.</div>
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/image4.png" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          <div class="content has-text-justified">Comparison of the baseline VLA model (Octo) and the VLA model after
            collaborative learning (tuning). The success rates of the fine-tuned VLA model—with and without the
            rule-based expert policy (V vs. V-R)—are presented at the task level (a) and at the average level (b) in the
            MT10 benchmark.</div>
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/image8.png" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          <div class="content has-text-justified">Application of the collaboration framework in SSVEP-based BCI: A
            comparison between pure SSVEP-based control and the collaboration between the VLA model and the BCI user.
            Although in some cases the policy of the human participant performs better than the VLA model (steps: 77 vs.
            32), the collaboration system significantly improves time efficiency for a given task (time: 15s vs. 96s).
          </div>
        </h2>
      </div>
    </div>
  </section>


  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container"> -->
  <!-- Paper video. -->
  <!-- <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video"> -->
  <!-- Youtube embed code here -->
  <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
  <!-- Your video file here -->
  <!-- <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
  <!-- Your video file here -->
  <!-- <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
  <!-- Your video file here -->
  <!-- <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End video carousel -->

  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{xiang2025vlamodelexpertcollaborationbidirectional,
    title={VLA Model-Expert Collaboration for Bi-directional Manipulation Learning}, 
    author={Tian-Yu Xiang and Ao-Qun Jin and Xiao-Hu Zhou and Mei-Jiang Gui and Xiao-Liang Xie and Shi-Qi Liu and Shuang-Yi Wang and Sheng-Bin Duang and Si-Cheng Wang and Zheng Lei and Zeng-Guang Hou},
    year={2025},
    eprint={2503.04163},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/2503.04163}, 
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>